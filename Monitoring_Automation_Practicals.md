# ğŸš€ Unit VI â€“ Automation & Future Trends: Practical Guide

**For Absolute Beginners | Hands-On Practice**

---

## ğŸ“‹ Prerequisites Check

Before starting, verify your setup:

```bash
# Check Minikube is running
minikube status

# Check Prometheus pods
kubectl get pods -n default | grep prometheus

# Check Grafana pods
kubectl get pods -n default | grep grafana
```

Expected: All pods should show `Running` status âœ…

---

## ğŸ¯ What We'll Practice Today

1. âœ… **Backup & Restore Grafana Dashboards** (Export/Import)
2. âœ… **Configure Prometheus Data Retention** (Storage management)
3. âœ… **Basic Security Setup** (Authentication & HTTPS concepts)
4. âœ… **Install & Explore Grafana Loki** (Log aggregation)
5. âœ… **Troubleshoot Common Issues** (Real debugging)
6. âœ… **Understand OpenTelemetry** (Future standard)

---

## ğŸ“¦ Part 1: Understanding Your Current Setup with Helm

### ğŸ§  What is Helm? (Quick Recap)

Helm is like **App Store for Kubernetes**:
- Instead of creating 10+ YAML files manually
- Helm installs everything with 1 command
- You already used it to install Prometheus/Grafana!

### ğŸ” Let's See What Helm Installed

```bash
# List all Helm releases
helm list -A

# See what's in your Prometheus release
helm get values prometheus -n default

# See all resources created by Helm
helm get manifest prometheus -n default
```

**What you'll see**: Deployments, Services, ConfigMaps all created together!

---

## ğŸ’¾ Part 2: Backup & Restore Grafana Dashboards

### ğŸ§  Why Backup?

Imagine you spent 2 hours creating a beautiful dashboard... then accidentally deleted it! ğŸ˜±

**Solution**: Regular backups!

---

### ğŸ“¤ Method 1: Manual Backup (via UI)

**Step 1**: Access Grafana

```bash
# Get Grafana URL
minikube service grafana --url

# Or port-forward
kubectl port-forward svc/grafana 3000:80 -n default
```

Open: `http://localhost:3000`

**Step 2**: Export Dashboard
1. Login to Grafana (default: admin/admin)
2. Open any dashboard
3. Click âš™ï¸ (Settings) â†’ JSON Model
4. Copy the entire JSON
5. Save to a file: `dashboard-backup.json`

---

### ğŸ“¥ Restore Dashboard

**Step 1**: Go to Grafana UI
**Step 2**: Click `+` â†’ Import
**Step 3**: Paste JSON or upload file
**Step 4**: Click **Load** â†’ **Import**

âœ… Dashboard restored!

---

### ğŸ“¤ Method 2: Automated Backup (Command Line)

**Step 1**: Get Grafana admin password

```bash
# Find Grafana secret
kubectl get secret grafana -n default -o jsonpath="{.data.admin-password}" | base64 --decode
echo
```

**Step 2**: Create backup script

```bash
# Create backup directory
mkdir grafana-backups
cd grafana-backups

# Get Grafana URL
GRAFANA_URL="http://localhost:3000"
GRAFANA_USER="admin"
GRAFANA_PASS="your-password-here"

# List all dashboards (you'll need curl or a REST client)
# This is a preview - in production, teams use automation tools
```

**Note**: For beginners, UI method is easier. In industry, teams use tools like:
- Grafana API scripts
- Git repositories (store dashboards as code)
- Backup operators

---

### ğŸ§ª Practice Exercise

1. Create a simple dashboard in Grafana
2. Add a panel showing `up` metric
3. Export it as JSON
4. Delete the dashboard
5. Import it back

**You've mastered backup/restore!** ğŸ‰

---

## ğŸ—‚ï¸ Part 3: Managing Prometheus Data Retention

### ğŸ§  What is Data Retention?

**Simple explanation**: How long Prometheus keeps your metrics

Example:
- **7 days retention** â†’ Metrics older than 7 days are deleted
- **30 days retention** â†’ Metrics kept for 30 days

---

### âš–ï¸ Why It Matters

| Short Retention (7 days) | Long Retention (90 days) |
|-------------------------|--------------------------|
| âœ… Less disk space       | âŒ More disk space       |
| âœ… Faster queries        | âŒ Slower queries        |
| âŒ Less history          | âœ… More analysis         |

---

### ğŸ” Check Current Retention

```bash
# Check Prometheus configuration
kubectl get configmap prometheus-server -n default -o yaml | grep retention
```

---

### âš™ï¸ Configure Retention (Practical)

**Step 1**: Check current Prometheus deployment

```bash
kubectl get deployment prometheus-server -n default -o yaml | grep -A 5 "args:"
```

You'll see something like:
```yaml
args:
  - --storage.tsdb.retention.time=15d
  - --config.file=/etc/prometheus/prometheus.yml
```

**Step 2**: Update retention using Helm

```bash
# Update to 7 days retention
helm upgrade prometheus prometheus-community/prometheus \
  --set server.retention=7d \
  -n default

# Or 30 days
helm upgrade prometheus prometheus-community/prometheus \
  --set server.retention=30d \
  -n default
```

**Step 3**: Verify the change

```bash
kubectl get deployment prometheus-server -n default -o yaml | grep retention
```

---

### ğŸ§® Calculate Storage Needed

**Simple formula**:
```
Storage = Metrics per second Ã— Retention days Ã— 24 hours Ã— 3600 seconds Ã— ~2 bytes
```

**Example**:
- 10,000 metrics/sec
- 15 days retention
- â‰ˆ **26 GB** needed

---

### ğŸ§ª Practice Exercise

1. Check your current retention
2. Change it to 3 days (for testing)
3. Verify the change
4. Change it back to 15 days

---

## ğŸ” Part 4: Securing Monitoring Tools

### ğŸ§  Security 3 Pillars

```
Authentication â†’ Who are you?
Authorization  â†’ What can you do?
Encryption     â†’ Protect data in transit
```

---

### ğŸ”‘ Practice 1: Change Grafana Admin Password

**Step 1**: Access Grafana pod

```bash
# Get Grafana pod name
kubectl get pods -n default | grep grafana

# Example: grafana-5c7d4f8b6d-xyz12
POD_NAME="grafana-xxxxx"

# Access the pod
kubectl exec -it $POD_NAME -n default -- /bin/sh
```

**Step 2**: Inside the pod, use Grafana CLI

```bash
# Change admin password
grafana-cli admin reset-admin-password NewSecurePassword123

# Exit pod
exit
```

**Step 3**: Test new password in browser

---

### ğŸ›‚ Practice 2: Create Different User Roles

**Step 1**: Login to Grafana as admin

**Step 2**: Create users with different roles

1. Go to âš™ï¸ (Configuration) â†’ Users
2. Click **Invite**
3. Add email: `viewer@example.com`
4. Role: **Viewer** (can only view dashboards)
5. Click **Submit**

**Roles explained**:
- **Viewer** â†’ Read-only access
- **Editor** â†’ Can edit dashboards
- **Admin** â†’ Full control

---

### ğŸ” Practice 3: Enable HTTPS (Concept)

**In production, you'd use**:
```bash
# Create TLS certificate (example - don't run in Minikube)
kubectl create secret tls grafana-tls \
  --cert=path/to/cert.pem \
  --key=path/to/key.pem \
  -n default
```

**Then configure Grafana to use it**

**For Minikube**: Use port-forward with HTTPS is complex. In real environments:
- Use **Ingress** with TLS
- Use **cert-manager** for automatic certificates
- Use cloud load balancers with SSL

---

### ğŸ§ª Security Best Practices (Checklist)

```
âœ… Change default passwords
âœ… Use strong passwords (12+ characters)
âœ… Enable HTTPS in production
âœ… Use role-based access (RBAC)
âœ… Regularly update credentials
âœ… Audit access logs
```

---

## ğŸ“œ Part 5: Introduction to Grafana Loki

### ğŸ§  What is Grafana Loki?

**Simple explanation**: Loki is like Prometheus, but for **logs instead of metrics**

| Tool       | What it stores | Example                    |
|------------|----------------|----------------------------|
| Prometheus | Numbers        | CPU: 80%, Memory: 2GB      |
| Loki       | Text logs      | "Error: Connection failed" |

---

### ğŸ¯ Why Do We Need Loki?

**Problem**: You see CPU is high in Prometheus, but **why?**

**Solution**: Check logs in Loki!

```
Prometheus says: CPU = 95%
         â†“
Loki shows: "OutOfMemoryError in app.jar"
         â†“
You know the problem!
```

---

### ğŸ§© How Loki Works

```
Application writes logs
         â†“
Promtail collects logs (like a log agent)
         â†“
Loki stores logs (efficiently)
         â†“
Grafana displays logs (same UI!)
```

---

### ğŸš€ Install Grafana Loki (Practical)

**Step 1**: Add Loki Helm repository

```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
```

**Step 2**: Install Loki stack (Loki + Promtail)

```bash
# Install Loki with minimal resources (for Minikube)
helm install loki grafana/loki-stack \
  --set loki.persistence.enabled=false \
  --set promtail.enabled=true \
  -n default
```

**What this installs**:
- **Loki**: Log storage system
- **Promtail**: Log collector (runs on each node)

---

**Step 3**: Verify installation

```bash
# Check Loki pods
kubectl get pods -n default | grep loki

# You should see:
# loki-0                    1/1     Running
# loki-promtail-xxxxx       1/1     Running
```

---

### ğŸ”Œ Connect Loki to Grafana

**Step 1**: Access Grafana UI

```bash
kubectl port-forward svc/grafana 3000:80 -n default
```

**Step 2**: Add Loki as data source

1. Login to Grafana
2. Go to âš™ï¸ (Configuration) â†’ Data Sources
3. Click **Add data source**
4. Select **Loki**
5. Enter URL: `http://loki:3100`
6. Click **Save & Test**

âœ… You should see "Data source is working"

---

### ğŸ” Query Logs in Grafana

**Step 1**: Go to **Explore** (compass icon ğŸ§­)

**Step 2**: Select **Loki** data source

**Step 3**: Try these queries:

```logql
# Show all logs
{namespace="default"}

# Show logs from specific pod
{pod="prometheus-server-xxxxx"}

# Show error logs only
{namespace="default"} |= "error"

# Show logs with "failed"
{namespace="default"} |= "failed"
```

---

### ğŸ§ª Generate Some Logs (Testing)

```bash
# Create a test pod that generates logs
kubectl run log-generator \
  --image=busybox \
  --restart=Never \
  -- /bin/sh -c "while true; do echo 'Log message at $(date)'; sleep 5; done"

# View logs
kubectl logs -f log-generator

# Now check these logs in Grafana Loki!
# Query: {pod="log-generator"}
```

---

### ğŸ“Š Create Log Dashboard

**Step 1**: In Grafana, create new dashboard

**Step 2**: Add panel

**Step 3**: Select Loki data source

**Step 4**: Use query:
```logql
sum(count_over_time({namespace="default"}[1m]))
```

This shows: **Logs per minute**

---

### ğŸ“ Loki Summary (What You Learned)

```
âœ… Loki stores logs
âœ… Promtail collects logs from Kubernetes
âœ… Grafana displays both metrics AND logs
âœ… You can correlate: "CPU high" â†’ "Check logs" â†’ "Find error"
```

---

## ğŸ”­ Part 6: Understanding OpenTelemetry

### ğŸ§  What is OpenTelemetry (OTel)?

**The problem it solves**:

Before:
```
App â†’ Prometheus (metrics)
App â†’ Loki (logs)
App â†’ Jaeger (traces)
App â†’ DataDog (monitoring)
```

Each tool needs **different code**! ğŸ˜«

---

**After OpenTelemetry**:

```
App â†’ OpenTelemetry â†’ Any monitoring tool
```

**One standard, works everywhere!**

---

### ğŸ”º The 3 Pillars of Observability

| Pillar  | What it shows | Example                           |
|---------|---------------|-----------------------------------|
| Metrics | Numbers       | CPU: 80%                          |
| Logs    | Messages      | "Error: database timeout"         |
| Traces  | Request flow  | API â†’ Service A â†’ Service B â†’ DB |

OpenTelemetry handles **all three**!

---

### ğŸ¯ Why OpenTelemetry Matters (Real Example)

**Before OTel**:
```python
# Different library for each tool
import prometheus_client  # For metrics
import logging           # For logs
import jaeger_client     # For traces
```

**With OTel**:
```python
# One library for everything
from opentelemetry import trace, metrics, logs
```

---

### ğŸš€ OpenTelemetry Demo (Conceptual)

**How it works**:

```
Your Application
      â†“
OpenTelemetry SDK (installed in app)
      â†“
OpenTelemetry Collector (receives data)
      â†“
Sends to: Prometheus, Loki, Jaeger, etc.
```

---

### ğŸ§ª Install OpenTelemetry Collector (Optional Practice)

```bash
# Add OpenTelemetry Helm repo
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo update

# Install collector (minimal setup)
helm install otel-collector open-telemetry/opentelemetry-collector \
  --set mode=deployment \
  -n default
```

**Note**: This is advanced. For beginners, understanding the concept is enough!

---

### ğŸ“ OpenTelemetry Summary

```
âœ… One standard for metrics, logs, traces
âœ… Write instrumentation code once
âœ… Send data to multiple tools
âœ… Future of observability
âœ… Supported by: AWS, Google Cloud, Azure, Prometheus, Grafana
```

---

## ğŸ› ï¸ Part 7: Troubleshooting Common Issues

### âŒ Issue 1: No Data in Grafana

**Symptoms**: Dashboard shows "No Data"

**Debug Steps**:

```bash
# Step 1: Check Prometheus is running
kubectl get pods -n default | grep prometheus

# Step 2: Check Prometheus targets
kubectl port-forward svc/prometheus-server 9090:80 -n default
# Open: http://localhost:9090/targets
# All targets should be "UP"

# Step 3: Test query in Prometheus
# Open: http://localhost:9090/graph
# Run: up
# Should show results!

# Step 4: Check Grafana data source
# Grafana â†’ Data Sources â†’ Prometheus â†’ Test
```

---

### âŒ Issue 2: Prometheus Pod Crashing

**Symptoms**: Prometheus pod restarts frequently

**Debug Steps**:

```bash
# Check pod status
kubectl get pods -n default | grep prometheus

# Check logs
kubectl logs prometheus-server-xxxxx -n default

# Common causes:
# 1. Out of memory
# 2. Invalid configuration
# 3. Disk full
```

**Solution 1: Increase memory**

```bash
helm upgrade prometheus prometheus-community/prometheus \
  --set server.resources.limits.memory=2Gi \
  --set server.resources.requests.memory=1Gi \
  -n default
```

---

### âŒ Issue 3: Grafana Dashboard Slow

**Symptoms**: Dashboard takes 30+ seconds to load

**Causes**:
1. Query time range too large (30 days)
2. Too many metrics in one query
3. Missing indexes

**Solutions**:

```bash
# Solution 1: Reduce time range
# Change from "Last 30 days" â†’ "Last 6 hours"

# Solution 2: Use aggregation
# Instead of: rate(http_requests_total[5m])
# Use: avg(rate(http_requests_total[5m])) by (job)

# Solution 3: Add more resources to Prometheus
helm upgrade prometheus prometheus-community/prometheus \
  --set server.resources.limits.cpu=2 \
  -n default
```

---

### âŒ Issue 4: Loki Not Showing Logs

**Debug Steps**:

```bash
# Check Promtail is running
kubectl get pods -n default | grep promtail

# Check Promtail logs
kubectl logs loki-promtail-xxxxx -n default

# Check Loki is reachable
kubectl port-forward svc/loki 3100:3100 -n default
curl http://localhost:3100/ready

# Test Loki query
# Grafana â†’ Explore â†’ Loki â†’ {namespace="default"}
```

---

### ğŸ”§ Troubleshooting Checklist

```
âœ… Check pod status: kubectl get pods
âœ… Check pod logs: kubectl logs <pod-name>
âœ… Check service endpoints: kubectl get svc
âœ… Check configurations: kubectl get configmap
âœ… Test connectivity: kubectl port-forward
âœ… Verify resources: kubectl top pods
```

---

## ğŸŒ Part 8: AWS CloudWatch Integration (Concept)

### ğŸ§  What is AWS CloudWatch?

**CloudWatch** = AWS's monitoring service

When your Kubernetes runs on **AWS EKS**, CloudWatch automatically collects:
- EC2 instance metrics
- EKS cluster metrics
- Application logs

---

### ğŸ”— How It Connects

```
EKS Cluster
    â†“
CloudWatch (AWS metrics)
    â†“
Prometheus (scrapes CloudWatch Exporter)
    â†“
Grafana (displays everything)
```

---

### ğŸ†š CloudWatch vs Prometheus

| Feature          | CloudWatch      | Prometheus  |
|------------------|-----------------|-------------|
| Where           | AWS only        | Anywhere    |
| Cost            | Pay per metric  | Free/Open   |
| Integration     | Deep AWS        | K8s native  |
| Best for        | AWS resources   | Applications|

**In practice**: Use **both together**!

---

### ğŸ“ Summary

```
âœ… CloudWatch = AWS monitoring
âœ… Best for AWS infrastructure
âœ… Can export to Prometheus
âœ… Combine with Grafana for unified view
```

---

## ğŸ”„ Part 9: End-to-End Monitoring Pipeline

### ğŸ¨ Complete Picture (What You Built)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Your Application                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€â†’ Metrics â”€â”€â†’ Prometheus
            â”‚                    â†“
            â”‚              (stores numbers)
            â”‚                    â†“
            â”‚               Alertmanager
            â”‚                    â†“
            â”‚              (sends alerts)
            â”‚
            â””â”€â”€â†’ Logs â”€â”€â”€â”€â†’ Promtail â”€â”€â†’ Loki
                                         â†“
                                    (stores logs)
                                         â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                       â”‚
                    â–¼                                       â–¼
              Grafana Dashboards                      Alert Notifications
              (metrics + logs)                        (email, Slack)
                    â”‚
                    â–¼
               Your Team ğŸ‘¥
```

---

### ğŸ¯ What Each Tool Does

```
Application      â†’ Generates metrics & logs
Prometheus       â†’ Collects & stores metrics
Loki             â†’ Collects & stores logs
Promtail         â†’ Ships logs to Loki
Alertmanager     â†’ Sends alerts
Grafana          â†’ Visualizes everything
```

---

### ğŸš€ Industry Best Practices

```
âœ… Use Helm for installation
âœ… Store configs in Git
âœ… Enable authentication
âœ… Set up HTTPS in production
âœ… Backup dashboards regularly
âœ… Monitor the monitoring system itself!
âœ… Set reasonable retention periods
âœ… Use labels for organization
âœ… Create runbooks for alerts
```

---

## ğŸ“ Part 10: Final Practice Exercises

### Exercise 1: Complete Monitoring Setup

```bash
# 1. Check all monitoring components
kubectl get pods -n default

# 2. Access each service
kubectl port-forward svc/prometheus-server 9090:80 -n default
kubectl port-forward svc/grafana 3000:80 -n default
kubectl port-forward svc/loki 3100:3100 -n default

# 3. Verify data flow
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000
# Loki: http://localhost:3100/ready
```

---

### Exercise 2: Create End-to-End Dashboard

**Goal**: One dashboard showing metrics AND logs

1. Create new dashboard in Grafana
2. Add panel: Prometheus metrics (CPU usage)
3. Add panel: Loki logs (from same pod)
4. Save dashboard
5. Export as JSON (backup!)

---

### Exercise 3: Simulate Issue & Debug

```bash
# Create a failing pod
kubectl run failing-app \
  --image=busybox \
  --restart=Never \
  -- /bin/sh -c "echo 'Starting app'; sleep 5; echo 'ERROR: Database connection failed'; exit 1"

# Now debug using your monitoring:
# 1. Check Prometheus: Is pod up?
# 2. Check Loki: What error did it show?
# 3. Check Grafana: Visualize the failure
```

---

## ğŸ“ What You've Mastered

```
âœ… Backup & restore Grafana dashboards
âœ… Configure Prometheus data retention
âœ… Basic security (passwords, roles)
âœ… Install & use Grafana Loki
âœ… Understand logs vs metrics vs traces
âœ… Troubleshoot monitoring issues
âœ… Understand OpenTelemetry (future standard)
âœ… Know AWS CloudWatch integration
âœ… Built complete monitoring pipeline
```

---

## ğŸš€ Next Steps (For Your Career)

### Beginner â†’ Intermediate

```
1. Learn PromQL deeply (query language)
2. Create custom exporters
3. Set up Alertmanager with real notifications
4. Implement distributed tracing (Jaeger)
5. Use Thanos for long-term storage
```

### Tools to Explore

```
- Thanos (Prometheus at scale)
- Cortex (Multi-tenant Prometheus)
- Tempo (Distributed tracing)
- VictoriaMetrics (Fast metrics DB)
- Mimir (Grafana's metrics backend)
```

---

## ğŸ“š Useful Commands Cheat Sheet

```bash
# Helm operations
helm list -A
helm upgrade <release> <chart>
helm rollback <release>

# Kubernetes debugging
kubectl get pods -n <namespace>
kubectl logs <pod-name>
kubectl describe pod <pod-name>
kubectl port-forward svc/<service> <local-port>:<remote-port>

# Prometheus
curl http://localhost:9090/api/v1/targets
curl http://localhost:9090/api/v1/query?query=up

# Loki
curl http://localhost:3100/ready
curl http://localhost:3100/metrics
```

---

## ğŸ‰ Congratulations!

You now understand:
- **Modern monitoring architecture**
- **Industry-standard tools**
- **Practical troubleshooting**
- **Future trends in observability**

**You're ready for real DevOps/SRE work!** ğŸš€

---

## ğŸ’¡ Remember

```
"Monitoring is not just about collecting data.
 It's about understanding your system
 and fixing problems before users notice them."
```

**Keep practicing, keep learning!** ğŸ¯
